{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n_5oRe0SXilM",
        "9SxiIczg1s1k",
        "ll5e8N9SVwVa",
        "5PbjhPxLmsI4",
        "ROFPolZpm21t",
        "qBXGs0xRERuv",
        "AD6zwuTHiYKA",
        "NVSRftm8X1m1",
        "pclZR6uFklf_",
        "8UQgU5I-lEll",
        "FJ5rjq7fIe8Q",
        "q7CNxkPdNB4L",
        "8S1jwU4cXQX4",
        "ITfbaOgfYNsq",
        "x2NWxK0BFwyw",
        "RHRXds9U9134",
        "K4qgOdz7Yyeb",
        "Vlf6_berQ1vq",
        "zI6s2Amob48j",
        "zZX9MQlORLfY",
        "AQ69XKdbZcA3",
        "7-CGSS2OZKHD",
        "Bxtv48o-F1Ku",
        "EHZ-hHGuY5aG",
        "puQNgKN0wS7H",
        "vhhycm2S6wbz",
        "0dG6U6s3T95t"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_5oRe0SXilM"
      },
      "source": [
        "# Introduction to machine learning & Data Analysis\n",
        "\n",
        "Basic introduction on how to perform typical machine learning tasks with Python.\n",
        "\n",
        "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
        "Data Science Lab, University Of Bern, 2022\n",
        "\n",
        "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n",
        "\n",
        "# Part 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SxiIczg1s1k"
      },
      "source": [
        "# What is Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll5e8N9SVwVa"
      },
      "source": [
        "## Why Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6tHZQCywhGB"
      },
      "source": [
        "\n",
        "\n",
        "1.   \n",
        "2.   \n",
        "3.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PbjhPxLmsI4"
      },
      "source": [
        "## Learning from data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsd1MyT9eIdW"
      },
      "source": [
        "Unlike classical algorithms, created by a human to analyze some data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtoqE5XO3L1j"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_1.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu4Uq4k_ePoo"
      },
      "source": [
        "in machine learning the data itself is used for to define the algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2xIgm223vfa"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_2.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvAyI1uzfBUT"
      },
      "source": [
        "Machine learning - is learnign from data.\n",
        "If \n",
        "* performance on a task **T**\n",
        "* improves according to measure **P**\n",
        "* with experience **E**\n",
        "\n",
        "we say that model learns form data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9YsgnbD32dk"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_3.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to8vOJeC1xjE"
      },
      "source": [
        "\n",
        "The boundary is a bit fuzzy.\n",
        "\n",
        "In fact when we create algorithms, the problem in hand (namely the data  related to the problem), drives us to choose one or another algorithm. And we then tune it, to perform well on a task in hand. \n",
        "\n",
        "Machine Learning (ML) formalized this procedure, allowing us to automate (part) of this process.\n",
        "\n",
        "In this course you will get acquainted with the basics of Machine Learning â€” where the approach to handling the data (the algorithm) is defined, or as we say \"learned\" from data in hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROFPolZpm21t"
      },
      "source": [
        "## Classification vs Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70_dMCX340Rm"
      },
      "source": [
        "The two main tasks handled by (supervised) ML is regression and classification.\n",
        "In regression we aim at modeling the relationship between the system's response (dependent variable) and one or more explanatory variables (independent variables).\n",
        "\n",
        "Examples of regression would be predicting the temperature for each day of the year, or expenses of the household as a function of the number of children and adults.\n",
        "\n",
        "In classification the aim is to identify what class does a data-point belong to. For example, the species or the iris plant based on the size of its petals, or whether an email is spam or not based on its content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBXGs0xRERuv"
      },
      "source": [
        "## Performance measures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx37P09Vkepw"
      },
      "source": [
        "1. Regression:\n",
        "* Mean Square Error: $\\textrm{MSE}=\\frac{1}{n}\\sum_i(y_i - \\hat y(\\bar x_i))^2$\n",
        "* Mean Absolute Error: $\\textrm{MAE}=\\frac{1}{n}\\sum_i|y_i - \\hat y(\\bar x_i)|$\n",
        "* Median Absolute Deviation: $\\textrm{MAD}=\\textrm{median}(|y_i - \\hat y(\\bar x_i)|)$\n",
        "* Fraction of the explained variance: $R^2=1-\\frac{\\sum_i(y_i - \\hat y(\\bar x_i))^2}{\\sum_i(y_i - \\bar y_i)^2}$, where $\\bar y=\\frac{1}{n}\\sum_i y_i$\n",
        "\n",
        "2. Classification:\n",
        "* Confusion matrix \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSH3blOw36jz"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/confusion_mtr.png\" width=\"46%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK2gGVJyfdUJ"
      },
      "source": [
        "* Accuracy $=\\frac{\\textrm{TP} + \\textrm{TN}}{\\textrm{TP} + \\textrm{FP} + \\textrm{FN} + \\textrm{TN}}$\n",
        "* Precision $=\\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FP}}$ \n",
        "* Recall $=\\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FN}}$\n",
        "* F1 $=\\frac{2}{\\frac{1}{\\textrm{Precision}} + \\frac{1}{\\textrm{Recall}} } = 2\\frac{\\textrm{Precision} \\cdot \\textrm{Recall}}{\\textrm{Precision} + \\textrm{Recall}} = \\frac{2 \\textrm{TP}}{2 \\textrm{TP} + \\textrm{FP} + \\textrm{FN}} = \\frac{\\textrm{TP}}{\\textrm{TP} + \\frac{\\textrm{FP} + \\textrm{FN}}{2}}$\n",
        "* F2 $=\\frac{5}{\\frac{1}{\\textrm{Precision}} + \\frac{4}{\\textrm{Recall}} } = 5\\frac{\\textrm{Precision} \\cdot \\textrm{Recall}}{4 \\cdot \\textrm{Precision} + \\textrm{Recall}} = \\frac{5 \\textrm{TP}}{5 \\textrm{TP} + \\textrm{FP} + 4 \\cdot \\textrm{FN}} = \\frac{\\textrm{TP}}{\\textrm{TP} + \\frac{\\textrm{FP} + 4 \\cdot\\textrm{FN}}{5}}$\n",
        "* Threat score (TS), or Intersection over Union: $\\mathrm{IoU}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}+\\mathrm{FP}}$\n",
        "\n",
        "\n",
        "During model optimization the used measure in most cases must be differentiable. To this end usually some measure of similarities of distributions are employed (e.g. cross-entropy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD6zwuTHiYKA"
      },
      "source": [
        "## Actual aim: Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNsD3FQS4JP7"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Bias_variance_1.png\" width=\"35%\"/>\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Bias_variance_2.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoONru7ji3QD"
      },
      "source": [
        "To measure model performance in an unbiassed way, we need to use different data than the data that the model was trained on. For this we use the 'train-test' split: e.g. 20% of all available dataset is reserved for model performance test, and the remaining 80% is used for actual model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVSRftm8X1m1"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVJn0ilgOS8F",
        "scrolled": true
      },
      "source": [
        "# Scikit-learn (formerly scikits.learn and also known as sklearn) is a free \n",
        "# software machine learning library for the Python programming language. \n",
        "# It features various classification, regression and clustering algorithms, \n",
        "# and is designed to interoperate with the Python numerical and scientific \n",
        "# libraries NumPy and SciPy. (from wiki)\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import tree\n",
        "from sklearn import ensemble\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# common visualization module\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# numeric library\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from time import time as timer\n",
        "import tarfile\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg0LDjc5nECH"
      },
      "source": [
        "pip install dtreeviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gFMn8yunHDI"
      },
      "source": [
        "from dtreeviz.trees import dtreeviz # remember to load the package"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y7aMevU3Ug8"
      },
      "source": [
        "if not os.path.exists('data'):\n",
        "    path = os.path.abspath('.')+'/colab_material.tgz'\n",
        "    tf.keras.utils.get_file(path, 'https://github.com/neworldemancer/DSF5/raw/master/colab_material.tgz')\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pclZR6uFklf_"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_wxOrdWko8W"
      },
      "source": [
        "In this course we will use several synthetic and real-world datasets to illustrate the behavior of the models and exercise our skills."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UQgU5I-lEll"
      },
      "source": [
        "## 1. Synthetic linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGfWOWRjlWPa"
      },
      "source": [
        "def get_linear(n_d=1, n_points=10, w=None, b=None, sigma=5):\n",
        "  x = np.random.uniform(0, 10, size=(n_points, n_d))\n",
        "  \n",
        "  w = w or np.random.uniform(0.1, 10, n_d)\n",
        "  b = b or np.random.uniform(-10, 10)\n",
        "  y = np.dot(x, w) + b + np.random.normal(0, sigma, size=n_points)\n",
        "\n",
        "  print('true slopes: w =', w, ';  b =', b)\n",
        "\n",
        "  return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RLYxGy_nBZG"
      },
      "source": [
        "x, y = get_linear(n_d=1, sigma=1)\n",
        "plt.plot(x[:, 0], y, '*')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ODDOp4nX4S"
      },
      "source": [
        "n_d = 2\n",
        "x, y = get_linear(n_d=n_d, n_points=100)\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x[:,0], x[:,1], y, marker='x', color='b',s=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ5rjq7fIe8Q"
      },
      "source": [
        "## 2. House prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-45usskInlD"
      },
      "source": [
        "Subset of the Ames Houses dataset: http://jse.amstat.org/v19n3/decock.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVv2ID96IyN0"
      },
      "source": [
        "def house_prices_dataset(return_df=False, return_df_xy=False, price_max=400000, area_max=40000):\n",
        "  path = 'data/AmesHousing.csv'\n",
        "\n",
        "  df = pd.read_csv(path, na_values=('NaN', ''), keep_default_na=False,  )\n",
        "  \n",
        "  rename_dict = {k:k.replace(' ', '').replace('/', '') for k in df.keys()}\n",
        "  df.rename(columns=rename_dict, inplace=True)\n",
        "  \n",
        "  useful_fields = ['LotArea',\n",
        "                  'Utilities', 'OverallQual', 'OverallCond',\n",
        "                  'YearBuilt', 'YearRemodAdd', 'ExterQual', 'ExterCond',\n",
        "                  'HeatingQC', 'CentralAir', 'Electrical',\n",
        "                  '1stFlrSF', '2ndFlrSF','GrLivArea',\n",
        "                  'FullBath', 'HalfBath',\n",
        "                  'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
        "                  'Functional','PoolArea',\n",
        "                  'YrSold', 'MoSold'\n",
        "                  ]\n",
        "  target_field = 'SalePrice'\n",
        "\n",
        "  df.dropna(axis=0, subset=useful_fields+[target_field], inplace=True)\n",
        "\n",
        "  cleanup_nums = {'Street':      {'Grvl': 0, 'Pave': 1},\n",
        "                  'LotFrontage': {'NA':0},\n",
        "                  'Alley':       {'NA':0, 'Grvl': 1, 'Pave': 2},\n",
        "                  'LotShape':    {'IR3':0, 'IR2': 1, 'IR1': 2, 'Reg':3},\n",
        "                  'Utilities':   {'ELO':0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n",
        "                  'LandSlope':   {'Sev':0, 'Mod': 1, 'Gtl': 3},\n",
        "                  'ExterQual':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'ExterCond':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'BsmtQual':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtCond':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtExposure':{'NA':0, 'No':1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
        "                  'BsmtFinType1':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'BsmtFinType2':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'HeatingQC':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'CentralAir':  {'N':0, 'Y': 1},\n",
        "                  'Electrical':  {'':0, 'NA':0, 'Mix':1, 'FuseP':2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5},\n",
        "                  'KitchenQual': {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Functional':  {'Sal':0, 'Sev':1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2':5, 'Min1':6, 'Typ':7},\n",
        "                  'FireplaceQu': {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'PoolQC':      {'NA':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Fence':       {'NA':0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv':4},\n",
        "                  }\n",
        "\n",
        "  df_X = df[useful_fields].copy()                              \n",
        "  df_X.replace(cleanup_nums, inplace=True)  # convert continous categorial variables to numerical\n",
        "  df_Y = df[target_field].copy()\n",
        "\n",
        "  x = df_X.to_numpy().astype(np.float32)\n",
        "  y = df_Y.to_numpy().astype(np.float32)\n",
        "\n",
        "  if price_max>0:\n",
        "    idxs = y<price_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  if area_max>0:\n",
        "    idxs = x[:,0]<area_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  return (x, y, df) if return_df else ((x, y, (df_X, df_Y)) if return_df_xy else (x,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqWU0eHts1RM"
      },
      "source": [
        "x, y, df = house_prices_dataset(return_df=True)\n",
        "print(x.shape, y.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDtzVS-1Mxxe"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91nj7znzMEpA"
      },
      "source": [
        "plt.plot(x[:, 0], y, '.')\n",
        "plt.xlabel('area, sq.ft')\n",
        "plt.ylabel('price, $');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7CNxkPdNB4L"
      },
      "source": [
        "## 3. Blobs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8wXhleONKgZ"
      },
      "source": [
        "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
        "colors = \"ygr\"\n",
        "for i, color in enumerate(colors):\n",
        "    idx = y == i\n",
        "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKcmdcZf0VO8"
      },
      "source": [
        "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
        "\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]  # affine transformation matrix\n",
        "x = np.dot(x, transformation)               # applied to point coordinated to make blobs less separable\n",
        "\n",
        "colors = \"ygr\"\n",
        "for i, color in enumerate(colors):\n",
        "    idx = y == i\n",
        "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S1jwU4cXQX4"
      },
      "source": [
        "## 4. MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2u82UQ5XQX4"
      },
      "source": [
        "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
        "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting (taken from http://yann.lecun.com/exdb/mnist/). Each example is a 28x28 grayscale image and the dataset can be readily downloaded from Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaNaGGOkXQX5"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlUY5gl8XQX7"
      },
      "source": [
        "Let's check few samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtYtGEDdXQX8"
      },
      "source": [
        "n = 3\n",
        "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
        "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
        "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
        "  im = train_images[im_idx]\n",
        "  im_class = train_labels[im_idx]\n",
        "  axi.imshow(im, cmap='gray')\n",
        "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
        "  axi.grid(False)\n",
        "plt.tight_layout(0,0,0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITfbaOgfYNsq"
      },
      "source": [
        "## 5. Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgzzOS7YYTru"
      },
      "source": [
        "`Fashion-MNIST` is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (from https://github.com/zalandoresearch/fashion-mnist)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcV2gzmuYljJ"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPw6-GoPbT6U"
      },
      "source": [
        "Let's check few samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHFd0sFHY4Li"
      },
      "source": [
        "n = 3\n",
        "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
        "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
        "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
        "  im = train_images[im_idx]\n",
        "  im_class = train_labels[im_idx]\n",
        "  axi.imshow(im, cmap='gray')\n",
        "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
        "  axi.grid(False)\n",
        "plt.tight_layout(0,0,0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2LkoWfZEi4g"
      },
      "source": [
        "fmnist_class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHEA0tCLagoV"
      },
      "source": [
        "Each of the training and test examples is assigned to one of the following labels:\n",
        "\n",
        "| Label | Description |\n",
        "| --- | --- |\n",
        "| 0 | T-shirt/top |\n",
        "| 1 | Trouser |\n",
        "| 2 | Pullover |\n",
        "| 3 | Dress |\n",
        "| 4 | Coat |\n",
        "| 5 | Sandal |\n",
        "| 6 | Shirt |\n",
        "| 7 | Sneaker |\n",
        "| 8 | Bag |\n",
        "| 9 | Ankle boot |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Weather dataset"
      ],
      "metadata": {
        "id": "x2NWxK0BFwyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a weather time series [dataset](https://www.bgc-jena.mpg.de/wetter/) recorded by the Max Planck Institute for Biogeochemistry \n",
        "It contains weather reacord for 8 years of observation."
      ],
      "metadata": {
        "id": "LKsTkrMkGB7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weather_df():\n",
        "  # inspired by https://www.tensorflow.org/tutorials/structured_data/time_series\n",
        "\n",
        "  # download and extract dataset\n",
        "  zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "  csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "  # load into pandas df\n",
        "  df = pd.read_csv(csv_path)\n",
        "  \n",
        "  # dataset contains records every 10 min, we use hourly records only, thus\n",
        "  # slice [start:stop:step], starting from index 5 take every 6th record\n",
        "  df = df[5::6]\n",
        "\n",
        "  # replace errors in wind velocity to 0\n",
        "  wv = df['wv (m/s)']\n",
        "  bad_wv = wv == -9999.0\n",
        "  wv[bad_wv] = 0.0\n",
        "\n",
        "  max_wv = df['max. wv (m/s)']\n",
        "  bad_max_wv = max_wv == -9999.0\n",
        "  max_wv[bad_max_wv] = 0.0\n",
        "\n",
        "  # obtain timestamps from text time format\n",
        "  date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
        "  timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
        "  # genarate cyclic features for year and day\n",
        "  day = 24*60*60\n",
        "  year = (365.2425) * day\n",
        "\n",
        "  df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
        "  df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
        "  df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
        "  df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "D_bXLpwxGTbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df = get_weather_df()"
      ],
      "metadata": {
        "id": "DsYDOZZKI7_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df.head()"
      ],
      "metadata": {
        "id": "yQHfhrBMJA5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df.describe().T"
      ],
      "metadata": {
        "id": "wPUvJcY3JjJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(weather_df['Year cos'])\n",
        "plt.plot(weather_df['Year sin'])"
      ],
      "metadata": {
        "id": "T7qIhgZuMGW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(weather_df['Day cos'][:7*24])\n",
        "plt.plot(weather_df['Day sin'][:7*24])"
      ],
      "metadata": {
        "id": "WxziDnFwLqL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_future_T_dataset(X_len=24, Y_offset=48,\n",
        "                         X_features=['p (mbar)', 'T (degC)', 'rh (%)', 'wv (m/s)', 'wd (deg)', 'Day sin', 'Day cos', 'Year sin', 'Year cos'],\n",
        "                         Y_features='T (degC)',\n",
        "                         standardize=True,\n",
        "                         oversample=10\n",
        "                         ):\n",
        "  \"\"\"\n",
        "  Generates pairs of input-label, using sequence of `X_len` samples as input\n",
        "  and value at offset `Y_offset` from start of this sequence as label.\n",
        "  Sample sequnces arte taken at random positions throughout the dataset.\n",
        "  Number of samples is obtained assuming non-overlaping wondows.\n",
        "  Oversampling factor allows to increase this value.\n",
        "\n",
        "  Args:\n",
        "    X_len (int): length of sample sequence\n",
        "    Y_offset (int): offset to the target value from the sequence start\n",
        "    X_features (list or str): features to be used as input\n",
        "    Y_features (list or str): features to be used as labels\n",
        "    standardize (Bool): flag whether to standardize the columns\n",
        "    oversample (int): increases number of samples by this factor wrt baseline\n",
        "                      n = len(df) // (Y_offset+1)\n",
        "  \"\"\"\n",
        "  weather_df = get_weather_df()\n",
        "\n",
        "  if standardize:\n",
        "    mean = weather_df.mean()\n",
        "    std = weather_df.std()\n",
        "    weather_df = (weather_df - mean) / std\n",
        "  \n",
        "  df_X = weather_df[X_features]\n",
        "  df_Y = weather_df[Y_features]\n",
        "\n",
        "  n_records = len(df_X)\n",
        "  sample_len = Y_offset+1\n",
        "  n_samples = int((n_records-sample_len)//sample_len*oversample)\n",
        "  offsets = np.random.randint(0, n_records-sample_len, size=n_samples)\n",
        "  offsets.sort()\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "  for o in offsets:\n",
        "    X.append(np.array(df_X[o:o+X_len]))\n",
        "    Y.append(np.array(df_Y[o+Y_offset:o+Y_offset+1]))\n",
        "\n",
        "  X = np.stack(X)\n",
        "  Y = np.concatenate(Y)\n",
        "\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "tDQkIP4cJxuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = gen_future_T_dataset()"
      ],
      "metadata": {
        "id": "m-uVf1AOPLIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y.shape"
      ],
      "metadata": {
        "id": "m2f1pWLfPbJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f,fn in enumerate(['p (mbar)', 'T (degC)', 'rh (%)', 'wv (m/s)', 'wd (deg)', 'Day sin', 'Day cos', 'Year sin', 'Year cos']):\n",
        "  plt.figure(figsize=(5*(1+(f==1)), 4))\n",
        "  for s in range(10):\n",
        "    plt.plot(x[s, :, f])\n",
        "    if f==1:\n",
        "      plt.scatter(48, y[s], color=plt.gca().lines[-1].get_color())\n",
        "  plt.title(fn)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JJU5MRbdRDEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHRXds9U9134"
      },
      "source": [
        "# `scikit-learn` interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2toQKrAzH_U"
      },
      "source": [
        "In this course we will primarily use the `scikit-learn` module.\n",
        "You can find extensive documentation with examples in the [user guide](https://scikit-learn.org/stable/user_guide.html)\n",
        "\n",
        "The module contains A LOT of different machine learning methods, and here we will cover only few of them. What is great about `scikit-learn` is that it has a uniform and consistent interface. \n",
        "\n",
        "All the different ML approaches are implemented as classes with a set of same main methods:\n",
        "\n",
        "1. `fitter = ...`: Create fitter object.\n",
        "2. `fitter.fit(x, y[, sample_weight])`: Fit model to predict from list of smaples `x` a list of target values `y`.\n",
        "3. `y_pred = fitter.predict(X)`: Predict using the trained model.\n",
        "4. `s = fitter.score(x, y[, sample_weight])`: Obtain a relevant performance measure of the trained model.\n",
        "\n",
        "This allows one to easily replace one approach with another and find the best one for the problem at hand, by simply using a regression/classification object of another class, while the rest of the code can remain the same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqLR5-eQ2vtz"
      },
      "source": [
        "It is useful to know that generally in scikit-learn the input data is represented as a matrix $X$ of dimensions `n_samples x n_features` , whereas the supervised labels/values are stored in a matrix $Y$ of dimensions `n_samples x n_target` ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4qgOdz7Yyeb"
      },
      "source": [
        "# 1.Linear models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh6lII-Hz8u-"
      },
      "source": [
        "In many cases the scalar value of interest - dependent variable - is (or can be approximated as) linear combination of the independent variables. \n",
        "\n",
        "In linear regression the estimator is searched in the form: $$\\hat{y}(\\bar{x} | \\bar{w}) = w_0 + w_1 x_1 + ... + w_p x_p$$\n",
        "\n",
        "The parameters $\\bar{w} = (w_1,..., w_p)$ and $w_0$ are designated as `coef_` and `intercept_` in `sklearn`.\n",
        "\n",
        "Reference: https://scikit-learn.org/stable/modules/linear_model.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlf6_berQ1vq"
      },
      "source": [
        "## 1. Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zatxRr8bOuTs"
      },
      "source": [
        "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) fits a linear model with coefficients $\\bar{w} = (w_1,..., w_p)$ and $w_0$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
        "\n",
        "Mathematically it solves a problem of the form:  $$\\bar{w} = \\min_{w_i} || \\bar{X} \\cdot \\bar{w} - y||_2^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqh7XwGkNg6r"
      },
      "source": [
        "x, y = get_linear(n_d=1, sigma=3, n_points=30)  # p==1, 1D input\n",
        "plt.scatter(x, y);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFawJfQJOKX3"
      },
      "source": [
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diHNLTNMOek5"
      },
      "source": [
        "w, w0 = reg.coef_, reg.intercept_\n",
        "print(w, w0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyeHY3bxPYSF"
      },
      "source": [
        "plt.scatter(x, y, marker='*', label='data points')\n",
        "x_f = np.linspace(x.min(), x.max(), 10)\n",
        "y_f = w0 + w[0] * x_f\n",
        "plt.plot(x_f, y_f, label='fit', c='r')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNX-5gYOIi40"
      },
      "source": [
        "# mse\n",
        "np.std(y - reg.predict(x))  # or use metrics.mean_squared_error(..., squared=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID0Hdzx0NvxF"
      },
      "source": [
        "# R2\n",
        "reg.score(x, y) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rg2_DZCHgJE"
      },
      "source": [
        "Let's try 2D input. \n",
        "Additionally, here we will split the whole dataset into training and test subsets using the `train_test_split` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK5MILosSI7d"
      },
      "source": [
        "n_d = 2\n",
        "x, y = get_linear(n_d=n_d, n_points=1000, sigma=5)\n",
        "\n",
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x_train[:,0], x_train[:,1], y_train, marker='x', s=40)\n",
        "ax.scatter(x_test[:,0], x_test[:,1], y_test, marker='+', s=80)\n",
        "\n",
        "xx0 = np.linspace(x[:,0].min(), x[:,0].max(), 10)\n",
        "xx1 = np.linspace(x[:,1].min(), x[:,1].max(), 10)\n",
        "xx0, xx1 = [a.flatten() for a in np.meshgrid(xx0, xx1)]\n",
        "xx = np.stack((xx0, xx1), axis=-1)\n",
        "yy = reg.predict(xx)\n",
        "ax.plot_trisurf(xx0, xx1, yy, alpha=0.25, linewidth=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW5GiLlhS3Y8"
      },
      "source": [
        "# mse\n",
        "print('train mse =', np.std(y_train - reg.predict(x_train)))\n",
        "print('test mse =', np.std(y_test - reg.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Fb1zb5S3ZG"
      },
      "source": [
        "# R2\n",
        "print('train R2 =', reg.score(x_train, y_train))\n",
        "print('test R2 =', reg.score(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI6s2Amob48j"
      },
      "source": [
        "## EXERCISE 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRi8SPiMb9FM"
      },
      "source": [
        "Use linear regression to fit house prices dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaQVHyvPcHW2"
      },
      "source": [
        "# 1. make train/test split\n",
        "\n",
        "# 2. fit the model\n",
        "\n",
        "# 3. evaluate MSE, MAD, and R2 on train and test datasets\n",
        "\n",
        "# 4. plot y vs predicted y for test and train parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZX9MQlORLfY"
      },
      "source": [
        "## 2. Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRUwQD5UR0Vf"
      },
      "source": [
        "Logistic regression, despite its name, is a linear model for classification rather than regression. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
        "\n",
        "In binary logistic regression the probability $p$ of a point belonging to a class is modeled as: $$\\frac{p}{1-p} = e^{w_0 + w_1 x_1 + ... + w_p x_p}$$\n",
        "\n",
        "\\\\\n",
        "In case of $N$ classes 2 options are available: \n",
        "* One-versus-rest (`ovr`) performs $N$ independent binary logistic regressions for each class $i$ just as for the binary case: $$\\frac{p_i}{1-p_i} = e^{w_{i0} + w_{i1} x_1 + ... + w_{ip} x_p}$$\n",
        "Final probability is obtained by evaluating the probability for all classes, and then normalizing it: $$\\hat p_i = \\frac{p_i}{\\sum_j{p_j}}$$\n",
        "\n",
        "\\\\\n",
        "* Multinimial (`multinomial`) logistic regression fits probability for each class $i$ with respect to a pivot, e.g. class $0$:\n",
        "$$\\frac{p_i}{p_0} = e^{w_{i0} + w_{i1} x_1 + ... + w_{ip} x_p},\\, i\\neq0$$\n",
        "Then softmax function is used to find the predicted probability of each class.\n",
        "\n",
        "\\\\\n",
        "The binary class $\\ell_2$-penalized logistic regression minimizes the following cost function:\n",
        "$$\\min_{w, c}  \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) + \\lambda \\frac{1}{2}w^T w$$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BnNRNDj-zbE"
      },
      "source": [
        "# routine for coloring 2d space according to class prediction\n",
        "\n",
        "def plot_prediction_2d(x_min, x_max, y_min, y_max, classifier, ax=None):\n",
        "  \"\"\"\n",
        "  Creates 2D mesh, predicts class for each point on the mesh, and visualises it\n",
        "  \"\"\"\n",
        "\n",
        "  mesh_step = .02  # step size in the mesh\n",
        "  x_coords = np.arange(x_min, x_max, mesh_step) # coordinates of mesh colums\n",
        "  y_coords = np.arange(y_min, y_max, mesh_step) # coordinates of mesh rows\n",
        "  \n",
        "  # create mesh, and get x and y coordinates of each point point\n",
        "  # arrenged as array of shape (n_mesh_rows, n_mesh_cols)\n",
        "  mesh_nodes_x, mesh_nodes_y = np.meshgrid(x_coords, y_coords)\n",
        "\n",
        "  # Plot the decision boundary. For that, we will assign a color to each\n",
        "  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "\n",
        "  # prepare xy pairs for prediction: matrix of size (n_mesh_rows*n_mesh_cols, 2)\n",
        "  mesh_xy_coords = np.stack([mesh_nodes_x.flatten(), \n",
        "                             mesh_nodes_y.flatten()], axis=-1)\n",
        "  \n",
        "  # obtain class for each node\n",
        "  mesh_nodes_class = classifier.predict(mesh_xy_coords)\n",
        "  \n",
        "  # reshape to the shape (n_mesh_rows, n_mesh_cols)==mesh_nodes_x.shape for visualization\n",
        "  mesh_nodes_class = mesh_nodes_class.reshape(mesh_nodes_x.shape)\n",
        "  \n",
        "  # Put the result into a color countour plot\n",
        "  ax = ax or plt.gca()\n",
        "  ax.contourf(mesh_nodes_x,\n",
        "              mesh_nodes_y,\n",
        "              mesh_nodes_class,\n",
        "              cmap='Pastel1', alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# routine for plotting 3d probability hyperplanes\n",
        "\n",
        "def plot_hyperplanes_3d(x_min, x_max, y_min, y_max, classifier):\n",
        "  \"\"\"\n",
        "  Creates 2D mesh, predicts class for each point on the mesh, and visualises it\n",
        "  \"\"\"\n",
        "\n",
        "  mesh_step = .5  # step size in the mesh\n",
        "  x_coords = np.arange(x_min, x_max, mesh_step) # coordinates of mesh colums\n",
        "  y_coords = np.arange(y_min, y_max, mesh_step) # coordinates of mesh rows\n",
        "  \n",
        "  # create mesh, and get x and y coordinates of each point point\n",
        "  # arrenged as array of shape (n_mesh_rows, n_mesh_cols)\n",
        "  mesh_nodes_x, mesh_nodes_y = np.meshgrid(x_coords, y_coords)\n",
        "\n",
        "  # Plot the decision boundary. For that, we will assign a color to each\n",
        "  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "\n",
        "  # prepare xy pairs for prediction: matrix of size (n_mesh_rows*n_mesh_cols, 2)\n",
        "  mesh_xy_coords = np.stack([mesh_nodes_x.flatten(), \n",
        "                             mesh_nodes_y.flatten()], axis=-1)\n",
        "  \n",
        "  # obtain class for each node\n",
        "  coef = classifier.coef_\n",
        "  intercept = classifier.intercept_\n",
        "\n",
        "  df_dict = {\n",
        "      'x':[],\n",
        "      'y':[],\n",
        "      'p':[],\n",
        "      'c':[]\n",
        "  }\n",
        "  def fill_dict(x, y, p, c):\n",
        "    df_dict['x'].append(x)\n",
        "    df_dict['y'].append(y)\n",
        "    df_dict['p'].append(p)\n",
        "    df_dict['c'].append(c)\n",
        "\n",
        "  for c in classifier.classes_:\n",
        "    coef_c = coef[c]\n",
        "    intercept_c = intercept[c]\n",
        "    for x, y in mesh_xy_coords:\n",
        "      p = x*coef_c[0] + y*coef_c[1] + intercept_c\n",
        "      fill_dict(x, y, p, c)\n",
        "  df = pd.DataFrame(df_dict)\n",
        "  fig = px.scatter_3d(df, x='x', y='y', z='p', color='c', size_max=1)\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "8-GFZDsykukn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJHWawkq0ev"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "x, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "x = np.dot(x, transformation)\n",
        "\n",
        "for multi_class in ('multinomial', 'ovr'):\n",
        "    # do fit\n",
        "    clf = linear_model.LogisticRegression(solver='sag', max_iter=100,\n",
        "                             multi_class=multi_class, )\n",
        "    clf.fit(x, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training accuracy : %.3f (%s)\" % (clf.score(x, y), multi_class))\n",
        "\n",
        "    # get range for visualization\n",
        "    x_0 = x[:, 0]\n",
        "    x_1 = x[:, 1]\n",
        "    x_min = x_0.min() - 1\n",
        "    x_max = x_0.max() + 1\n",
        "    y_min = x_1.min() - 1\n",
        "    y_max = x_1.max() + 1\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=clf)\n",
        "\n",
        "    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n",
        "    plt.axis('tight')\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = 'rbg'\n",
        "\n",
        "    for i, color in zip(clf.classes_, colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(x_0[idx], x_1[idx], c=color, cmap=plt.cm.Paired, \n",
        "                    edgecolor='gray', s=30, linewidth=0.2)\n",
        "\n",
        "    # Plot the three one-against-all classifiers\n",
        "    coef = clf.coef_\n",
        "    intercept = clf.intercept_\n",
        "\n",
        "    def plot_hyperplane(c, color):\n",
        "        def line(x0):\n",
        "            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
        "        plt.plot([x_min, x_max], [line(x_min), line(x_max)],\n",
        "                 ls=\"--\", color=color)\n",
        "\n",
        "    for i, color in zip(clf.classes_, colors):\n",
        "        plot_hyperplane(i, color)\n",
        "\n",
        "    plt.show()\n",
        "    #plot_hyperplanes_3d(x_min, x_max, y_min, y_max, classifier=clf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ69XKdbZcA3"
      },
      "source": [
        "## EXERCISE 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__9jcqXzZaQp"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRBuV1m_E4Ll"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "304Ul40adUT2"
      },
      "source": [
        "We will reshape 2-d images to 1-d arrays for use in scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtD8C8_4a7dP"
      },
      "source": [
        "n_train = len(train_labels)\n",
        "x_train = train_images.reshape((n_train, -1))\n",
        "y_train = train_labels\n",
        "\n",
        "n_test = len(test_labels)\n",
        "x_test = test_images.reshape((n_test, -1))\n",
        "y_test = test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJj7ofWD_Wp2"
      },
      "source": [
        "Now use a multinomial logistic regression classifier, and measure the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeIKcMeV_rmk"
      },
      "source": [
        "# 1. Create classifier\n",
        "\n",
        "# 2. fit the model\n",
        "\n",
        "# 3. evaluate accuracy on train and test datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-CGSS2OZKHD"
      },
      "source": [
        "\n",
        "# 2. Trees & Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxtv48o-F1Ku"
      },
      "source": [
        "## 1. Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l582Sr0_WGXj"
      },
      "source": [
        "Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning **simple** decision rules inferred from the data features.\n",
        "\n",
        "They are fast to train, easily interpretable, capture non-linear dependencies, and require small amount of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz5raIG_WQfg"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "dtcs = []\n",
        "for depth in (1, 2, 3, 4):\n",
        "    # do fit\n",
        "    dtc = tree.DecisionTreeClassifier(max_depth=depth, criterion='gini')  # 'entropy'\n",
        "    dtcs.append(dtc)\n",
        "    dtc.fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (depth=%d)\" % (dtc.score(X, y), depth))\n",
        "\n",
        "    # get range for visualization\n",
        "    x_0 = X[:, 0]\n",
        "    x_1 = X[:, 1]\n",
        "    x_min = x_0.min() - 1\n",
        "    x_max = x_0.max() + 1\n",
        "    y_min = x_1.min() - 1\n",
        "    y_max = x_1.max() + 1\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2,  figsize=(14,7), dpi=300)\n",
        "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=dtc, ax=ax[0])\n",
        "    \n",
        "    ax[0].set_title(\"Decision surface of DTC (%d)\" % depth)\n",
        "    \n",
        "    # Plot also the training points\n",
        "    colors = \"rbg\"\n",
        "    for i, color in zip(dtc.classes_, colors):\n",
        "        idx = np.where(y == i)\n",
        "        ax[0].scatter(x_0[idx], x_1[idx], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "\n",
        "    with plt.style.context('classic'):\n",
        "      tree.plot_tree(dtc, ax=ax[1]);\n",
        "\n",
        "    plt.tight_layout(0.5,0)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IgYhWghyUhL"
      },
      "source": [
        "Given fraction of samples of class $i$ as $p_i$\n",
        "\n",
        "* the Gini index is:\n",
        "$$G = 1 - \\sum_i {p_i^2}, $$\n",
        "\n",
        "* the Entropy is:\n",
        "$$E =  - \\sum_i {p_i * log(p_i)}, $$\n",
        "\n",
        "The optimization is performed in a greedy manner, one split at a time, minimizing impurity in the descendant nodes $G=G_1+G_2$ when the Gini index is used, or maximizing informatin gain: $IG = E_{node} - E_1 - E_2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYpitlKczjki"
      },
      "source": [
        "text_representation = tree.export_text(dtcs[2])\n",
        "print(text_representation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRsuFUM2nlP5"
      },
      "source": [
        "for i, dtc in enumerate(dtcs):\n",
        "  viz = dtreeviz(dtc, X, y, feature_names=['x[0]', 'x[1]'])\n",
        "  viz.scale=1.2\n",
        "  display(viz)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJK1XADt94uK"
      },
      "source": [
        "Additionally we can directly inspect relevance of the input features for the classification (impurity based):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxEFpn2P9Uek"
      },
      "source": [
        "plt.plot(dtcs[2].feature_importances_, '.')\n",
        "plt.xlabel('feature')\n",
        "plt.ylabel('importance')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHZ-hHGuY5aG"
      },
      "source": [
        "## 2. Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zETTKyFmwTae"
      },
      "source": [
        "The `sklearn.ensemble` provides several ensemble algorithms. RandomForest is an averaging algorithm based on randomized decision trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.\n",
        "\n",
        "Individual decision trees typically exhibit high variance and tend to overfit.\n",
        "In random forests:\n",
        "* each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.\n",
        "* when splitting each node during the construction of a tree, the best split is found from a random subset of features, according to `max_features` parameter.\n",
        "\n",
        "The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant, hence yielding an overall better model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4hfPUqSZCbH"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "for n_est in (1, 4, 50):\n",
        "    # do fit\n",
        "    rfc = ensemble.RandomForestClassifier(max_depth=4, n_estimators=n_est,)\n",
        "    rfc.fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (n_est=%d)\" % (rfc.score(X, y), n_est))\n",
        "\n",
        "    # get range for visualization\n",
        "    x_0 = X[:, 0]\n",
        "    x_1 = X[:, 1]\n",
        "    x_min = x_0.min() - 1\n",
        "    x_max = x_0.max() + 1\n",
        "    y_min = x_1.min() - 1\n",
        "    y_max = x_1.max() + 1\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=rfc)\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = 'rbg'\n",
        "    for i, color in enumerate(colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(x_0[idx], x_1[idx], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "    \n",
        "    # Plot the three one-against-all classifiers\n",
        "    xmin, xmax = plt.xlim()\n",
        "    ymin, ymax = plt.ylim()\n",
        "\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY6EVEestysr"
      },
      "source": [
        "plt.figure(dpi=300)\n",
        "with plt.style.context('classic'):\n",
        "  tree.plot_tree(rfc.estimators_[20]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-YP_GTWsLZI"
      },
      "source": [
        "  viz = dtreeviz(rfc.estimators_[20], X, y, feature_names=[f'{i}' for i in np.arange(len(X[0]))])\n",
        "  viz.scale=1.2\n",
        "  display(viz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLruRG-g_pyD"
      },
      "source": [
        "For a forest we can also evaluate the feature importance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks42i4mq-RM-"
      },
      "source": [
        "importances = np.array([e.feature_importances_ for e in rfc.estimators_])\n",
        "\n",
        "#plt.plot(importances.T, '.')\n",
        "plt.bar(['0', '1'], importances.mean(axis=0), yerr=importances.std(axis=0))\n",
        "plt.xlabel('feature')\n",
        "plt.ylabel('importance')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTNL6_57A0rq"
      },
      "source": [
        "Alternatively we can use permutation to study feature importance. \n",
        "It evaluates decrease in performance (model's `.score` or specified in `scoring` parameter) for each variable when its values is shuffled between samples. \n",
        "It can be time-consuming as requires re-evaluation for each feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrD0XYtI_8mQ"
      },
      "source": [
        "p_importances = permutation_importance(rfc, X, y, n_repeats=10, n_jobs=-1)\n",
        "\n",
        "plt.bar(['0', '1'],\n",
        "        p_importances.importances_mean,\n",
        "        yerr=p_importances.importances_std)\n",
        "plt.xlabel('feature')\n",
        "plt.ylabel('importance')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puQNgKN0wS7H"
      },
      "source": [
        "## 3. Boosted Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkL6_R05wez3"
      },
      "source": [
        "Another approach to the ensemble tree modeling is Boosted Decision Trees. In a boosting framework, the treees are created sequentially. This way each next tree reduces error of the ensamble, by fitting residuals of previous trees.\n",
        "\n",
        "Usually shallow trees are used in boosting framework. In boosting primarily the bias is reduced, thus increasing variance. Interpretability of this model is low.\n",
        "\n",
        "To avoid overfitting the learning rate and subsampling parameter can be tuned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qVKJ-YawFo9"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "for n_est in (1, 4, 50):\n",
        "    # do fit\n",
        "    dtc = ensemble.GradientBoostingClassifier(max_depth=1, n_estimators=n_est,\n",
        "                                              learning_rate=0.1, subsample=0.5)\n",
        "    dtc.fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (n_est=%d)\" % (dtc.score(X, y), n_est))\n",
        "\n",
        "    x_0 = X[:, 0]\n",
        "    x_1 = X[:, 1]\n",
        "    x_min = x_0.min() - 1\n",
        "    x_max = x_0.max() + 1\n",
        "    y_min = x_1.min() - 1\n",
        "    y_max = x_1.max() + 1\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=rfc)\n",
        "    \n",
        "    plt.title(f'Decision surface of DTC ({n_est})')\n",
        "    plt.axis('tight')\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = 'rbg'\n",
        "    for i, color in enumerate(colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(x_0[idx], x_1[idx], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "    \n",
        "    # Plot the three one-against-all classifiers\n",
        "    xmin, xmax = plt.xlim()\n",
        "    ymin, ymax = plt.ylim()\n",
        "\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhhycm2S6wbz"
      },
      "source": [
        "## EXERCISE 3 : Random forest classifier for FMNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I20uBLiMoURH"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "n = len(train_labels)\n",
        "x_train = train_images.reshape((n, -1))\n",
        "y_train = train_labels\n",
        "\n",
        "n_test = len(test_labels)\n",
        "x_test = test_images.reshape((n_test, -1))\n",
        "y_test = test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd59TNBWfHgX"
      },
      "source": [
        "Classify fashion MNIST images with Random Forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8waJCx33peIG"
      },
      "source": [
        "# 1. Create classifier. As the number of features is big (784), use bigger tree\n",
        "# depth (max_depth parameter), try in range 10-500.\n",
        "\n",
        "# 2. What is the maximum number of leaves in tree of depth n?\n",
        "# To reduce variance we should avoid leaves with too litle samples. You could\n",
        "# limit the total number of tree leaves (max_leaf_nodes parameter) to 10-1000.\n",
        "# Alternatively you can use min_samples_split & min_samples_leaf\n",
        "\n",
        "# 3. Try different number of estimators (n_estimators)\n",
        "\n",
        "# 4. Fit the model\n",
        "\n",
        "# 5. Inspect training and test accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dG6U6s3T95t"
      },
      "source": [
        "## EXERCISE 4: Random forest regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlypA1MaT95u"
      },
      "source": [
        "X, y, (df_x, df_y) = house_prices_dataset(return_df_xy=True)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYTwkoSpT95u"
      },
      "source": [
        "Predict the house prices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4wobzB_T95u"
      },
      "source": [
        "# 1. Create regressor. (ensemble.RandomForestRegressor)\n",
        "# Try different number of estimators (n_estimators)\n",
        "\n",
        "# 2. Fit the model\n",
        "\n",
        "# 3. Inspect training and test accuracy\n",
        "\n",
        "# 4. Try to improve performance by adjusting hyperparameters. \n",
        "# How does it compare to linear model?\n",
        "\n",
        "# 5. Use dtreeviz to visualize a tree from the ensamble\n",
        "\n",
        "# 6. Study the feature importance\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}