{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uRC8olfZCPU"
   },
   "source": [
    "# Introduction to machine learning & Data Analysis\n",
    "\n",
    "Basic introduction on how to perform typical machine learning tasks with Python.\n",
    "\n",
    "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
    "Data Science Lab, University Of Bern, 2022\n",
    "\n",
    "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n",
    "\n",
    "# Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-fYVr8NTLeK"
   },
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T14:05:44.613282Z",
     "start_time": "2022-09-27T14:05:43.538999Z"
    },
    "id": "hVJn0ilgOS8F",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 16:13:41.891222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from matplotlib import  pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.set()\n",
    "\n",
    "from time import time as timer\n",
    "from imageio import imread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T14:05:49.051625Z",
     "start_time": "2022-09-27T14:05:46.022444Z"
    },
    "id": "oes8tZkKXS1S"
   },
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T14:05:50.662474Z",
     "start_time": "2022-09-27T14:05:50.657850Z"
    },
    "id": "7xQIE54NmqJs"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    path = os.path.abspath('.')+'/colab_material.tgz'\n",
    "    tf.keras.utils.get_file(path, 'https://github.com/neworldemancer/DSF5/raw/master/colab_material.tgz')\n",
    "    tar = tarfile.open(path, \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T19:22:00.451788Z",
     "start_time": "2022-09-26T19:22:00.401516Z"
    },
    "id": "atch-QcelhRy"
   },
   "outputs": [],
   "source": [
    "from utils.routines import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBxkWZaLCUcR"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MssLYOiCUcX"
   },
   "source": [
    "In this course we will use several synthetic and real-world datasets to illustrate the behavior of the models and exercise our skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UQgU5I-lEll"
   },
   "source": [
    "## 1. Synthetic linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGfWOWRjlWPa"
   },
   "outputs": [],
   "source": [
    "def get_linear(n_d=1, n_points=10, w=None, b=None, sigma=5):\n",
    "  x = np.random.uniform(0, 10, size=(n_points, n_d))\n",
    "  \n",
    "  w = w or np.random.uniform(0.1, 10, n_d)\n",
    "  b = b or np.random.uniform(-10, 10)\n",
    "  y = np.dot(x, w) + b + np.random.normal(0, sigma, size=n_points)\n",
    "\n",
    "  print('true slopes: w =', w, ';  b =', b)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RLYxGy_nBZG"
   },
   "outputs": [],
   "source": [
    "x, y = get_linear(n_d=1, sigma=0)\n",
    "plt.plot(x[:, 0], y, '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10ODDOp4nX4S"
   },
   "outputs": [],
   "source": [
    "n_d = 2\n",
    "x, y = get_linear(n_d=n_d, n_points=100)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:,0], x[:,1], y, marker='x', color='b',s=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2uMw5C4CUcj"
   },
   "source": [
    "## 2. House prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi_Q6YTNCUcj"
   },
   "source": [
    "Subset of the Ames Houses dataset: http://jse.amstat.org/v19n3/decock.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj2LFlahCUcj"
   },
   "outputs": [],
   "source": [
    "def house_prices_dataset(return_df=False, price_max=400000, area_max=40000):\n",
    "  path = 'data/AmesHousing.csv'\n",
    "\n",
    "  df = pd.read_csv(path, na_values=('NaN', ''), keep_default_na=False)\n",
    "  \n",
    "  rename_dict = {k:k.replace(' ', '').replace('/', '') for k in df.keys()}\n",
    "  df.rename(columns=rename_dict, inplace=True)\n",
    "  \n",
    "  useful_fields = ['LotArea',\n",
    "                  'Utilities', 'OverallQual', 'OverallCond',\n",
    "                  'YearBuilt', 'YearRemodAdd', 'ExterQual', 'ExterCond',\n",
    "                  'HeatingQC', 'CentralAir', 'Electrical',\n",
    "                  '1stFlrSF', '2ndFlrSF','GrLivArea',\n",
    "                  'FullBath', 'HalfBath',\n",
    "                  'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
    "                  'Functional','PoolArea',\n",
    "                  'YrSold', 'MoSold'\n",
    "                  ]\n",
    "  target_field = 'SalePrice'\n",
    "\n",
    "  df.dropna(axis=0, subset=useful_fields+[target_field], inplace=True)\n",
    "\n",
    "  cleanup_nums = {'Street':      {'Grvl': 0, 'Pave': 1},\n",
    "                  'LotFrontage': {'NA':0},\n",
    "                  'Alley':       {'NA':0, 'Grvl': 1, 'Pave': 2},\n",
    "                  'LotShape':    {'IR3':0, 'IR2': 1, 'IR1': 2, 'Reg':3},\n",
    "                  'Utilities':   {'ELO':0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n",
    "                  'LandSlope':   {'Sev':0, 'Mod': 1, 'Gtl': 3},\n",
    "                  'ExterQual':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'ExterCond':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'BsmtQual':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
    "                  'BsmtCond':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
    "                  'BsmtExposure':{'NA':0, 'No':1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "                  'BsmtFinType1':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
    "                  'BsmtFinType2':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
    "                  'HeatingQC':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'CentralAir':  {'N':0, 'Y': 1},\n",
    "                  'Electrical':  {'':0, 'NA':0, 'Mix':1, 'FuseP':2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5},\n",
    "                  'KitchenQual': {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'Functional':  {'Sal':0, 'Sev':1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2':5, 'Min1':6, 'Typ':7},\n",
    "                  'FireplaceQu': {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
    "                  'PoolQC':      {'NA':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'Fence':       {'NA':0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv':4},\n",
    "                  }\n",
    "\n",
    "  df_X = df[useful_fields].copy()                              \n",
    "  df_X.replace(cleanup_nums, inplace=True)  # convert continous categorial variables to numerical\n",
    "  df_Y = df[target_field].copy()\n",
    "\n",
    "  x = df_X.to_numpy().astype(np.float32)\n",
    "  y = df_Y.to_numpy().astype(np.float32)\n",
    "\n",
    "  if price_max>0:\n",
    "    idxs = y<price_max\n",
    "    x = x[idxs]\n",
    "    y = y[idxs]\n",
    "\n",
    "  if area_max>0:\n",
    "    idxs = x[:,0]<area_max\n",
    "    x = x[idxs]\n",
    "    y = y[idxs]\n",
    "\n",
    "  return (x, y, df) if return_df else (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jcX7oYDFwzQ"
   },
   "outputs": [],
   "source": [
    "def house_prices_dataset_normed():\n",
    "    x, y = house_prices_dataset(return_df=False, price_max=-1, area_max=-1)\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "    features_scaled=scaler.fit_transform(x)\n",
    "    \n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqWU0eHts1RM"
   },
   "outputs": [],
   "source": [
    "x, y, df = house_prices_dataset(return_df=True)\n",
    "print(x.shape, y.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91nj7znzMEpA"
   },
   "outputs": [],
   "source": [
    "plt.plot(x[:, 0], y, '.')\n",
    "plt.xlabel('area, sq.ft')\n",
    "plt.ylabel('price, $');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7CNxkPdNB4L"
   },
   "source": [
    "## 3. Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8wXhleONKgZ"
   },
   "outputs": [],
   "source": [
    "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
    "colors = \"ygr\"\n",
    "for i, color in enumerate(colors):\n",
    "    idx = y == i\n",
    "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S1jwU4cXQX4"
   },
   "source": [
    "## 4. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2u82UQ5XQX4"
   },
   "source": [
    "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting (taken from http://yann.lecun.com/exdb/mnist/). Each example is a 28x28 grayscale image and the dataset can be readily downloaded from Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaNaGGOkXQX5"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlUY5gl8XQX7"
   },
   "source": [
    "Let's check few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtYtGEDdXQX8"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
    "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
    "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
    "  im = train_images[im_idx]\n",
    "  im_class = train_labels[im_idx]\n",
    "  axi.imshow(im, cmap='gray')\n",
    "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
    "  axi.grid(False)\n",
    "plt.tight_layout(0,0,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MXcNBOBCUcy"
   },
   "source": [
    "## 5. Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rANzstLwCUcy"
   },
   "source": [
    "`Fashion-MNIST` is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (from https://github.com/zalandoresearch/fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Woc5ld-HCUcz"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH2G-FAHCUc1"
   },
   "source": [
    "Let's check few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90EhR8nmCUc1"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
    "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
    "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
    "  im = train_images[im_idx]\n",
    "  im_class = train_labels[im_idx]\n",
    "  axi.imshow(im, cmap='gray')\n",
    "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
    "  axi.grid(False)\n",
    "plt.tight_layout(0,0,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeaJnq9hCUc3"
   },
   "source": [
    "Each of the training and test examples is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_wxOrdWko8W"
   },
   "source": [
    "In this course we will use several synthetic and real-world datasets to illustrate the behavior of the models and exercise our skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2NWxK0BFwyw"
   },
   "source": [
    "## 6. Weather dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKsTkrMkGB7d"
   },
   "source": [
    "This is a weather time series [dataset](https://www.bgc-jena.mpg.de/wetter/) recorded by the Max Planck Institute for Biogeochemistry \n",
    "It contains weather reacord for 8 years of observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_bXLpwxGTbZ"
   },
   "outputs": [],
   "source": [
    "def get_weather_df():\n",
    "  # inspired by https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "\n",
    "  # download and extract dataset\n",
    "  zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "  csv_path, _ = os.path.splitext(zip_path)\n",
    "\n",
    "  # load into pandas df\n",
    "  df = pd.read_csv(csv_path)\n",
    "  \n",
    "  # dataset contains records every 10 min, we use hourly records only, thus\n",
    "  # slice [start:stop:step], starting from index 5 take every 6th record\n",
    "  df = df[5::6]\n",
    "\n",
    "  # replace errors in wind velocity to 0\n",
    "  wv = df['wv (m/s)']\n",
    "  bad_wv = wv == -9999.0\n",
    "  wv[bad_wv] = 0.0\n",
    "\n",
    "  max_wv = df['max. wv (m/s)']\n",
    "  bad_max_wv = max_wv == -9999.0\n",
    "  max_wv[bad_max_wv] = 0.0\n",
    "\n",
    "  # obtain timestamps from text time format\n",
    "  date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "  timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "  # genarate cyclic features for year and day\n",
    "  day = 24*60*60\n",
    "  year = (365.2425) * day\n",
    "\n",
    "  df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "  df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "  df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "  df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsYDOZZKI7_P"
   },
   "outputs": [],
   "source": [
    "weather_df = get_weather_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQHfhrBMJA5w"
   },
   "outputs": [],
   "source": [
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPUvJcY3JjJa"
   },
   "outputs": [],
   "source": [
    "weather_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7qIhgZuMGW4"
   },
   "outputs": [],
   "source": [
    "plt.plot(weather_df['Year cos'])\n",
    "plt.plot(weather_df['Year sin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxziDnFwLqL0"
   },
   "outputs": [],
   "source": [
    "plt.plot(weather_df['Day cos'][:7*24])\n",
    "plt.plot(weather_df['Day sin'][:7*24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDQkIP4cJxuW"
   },
   "outputs": [],
   "source": [
    "def gen_future_T_dataset(X_len=24, Y_offset=48,\n",
    "                         X_features=['p (mbar)', 'T (degC)', 'rh (%)', 'wv (m/s)', 'wd (deg)', 'Day sin', 'Day cos', 'Year sin', 'Year cos'],\n",
    "                         Y_features='T (degC)',\n",
    "                         standardize=True,\n",
    "                         oversample=10\n",
    "                         ):\n",
    "  \"\"\"\n",
    "  Generates pairs of input-label, using sequence of `X_len` samples as input\n",
    "  and value at offset `Y_offset` from start of this sequence as label.\n",
    "  Sample sequnces arte taken at random positions throughout the dataset.\n",
    "  Number of samples is obtained assuming non-overlaping wondows.\n",
    "  Oversampling factor allows to increase this value.\n",
    "\n",
    "  Args:\n",
    "    X_len (int): length of sample sequence\n",
    "    Y_offset (int): offset to the target value from the sequence start\n",
    "    X_features (list or str): features to be used as input\n",
    "    Y_features (list or str): features to be used as labels\n",
    "    standardize (Bool): flag whether to standardize the columns\n",
    "    oversample (int): increases number of samples by this factor wrt baseline\n",
    "                      n = len(df) // (Y_offset+1)\n",
    "  \"\"\"\n",
    "  weather_df = get_weather_df()\n",
    "\n",
    "  if standardize:\n",
    "    mean = weather_df.mean()\n",
    "    std = weather_df.std()\n",
    "    weather_df = (weather_df - mean) / std\n",
    "  \n",
    "  df_X = weather_df[X_features]\n",
    "  df_Y = weather_df[Y_features]\n",
    "\n",
    "  n_records = len(df_X)\n",
    "  sample_len = Y_offset+1\n",
    "  n_samples = int((n_records-sample_len)//sample_len*oversample)\n",
    "  offsets = np.random.randint(0, n_records-sample_len, size=n_samples)\n",
    "  offsets.sort()\n",
    "\n",
    "  X = []\n",
    "  Y = []\n",
    "  for o in offsets:\n",
    "    X.append(np.array(df_X[o:o+X_len]))\n",
    "    Y.append(np.array(df_Y[o+Y_offset:o+Y_offset+1]))\n",
    "\n",
    "  X = np.stack(X)\n",
    "  Y = np.concatenate(Y)\n",
    "\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-uVf1AOPLIT"
   },
   "outputs": [],
   "source": [
    "x, y = gen_future_T_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2f1pWLfPbJG"
   },
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJU5MRbdRDEI"
   },
   "outputs": [],
   "source": [
    "for f,fn in enumerate(['p (mbar)', 'T (degC)', 'rh (%)', 'wv (m/s)', 'wd (deg)', 'Day sin', 'Day cos', 'Year sin', 'Year cos']):\n",
    "  plt.figure(figsize=(5*(1+(f==1)), 4))\n",
    "  for s in range(10):\n",
    "    plt.plot(x[s, :, f])\n",
    "    if f==1:\n",
    "      plt.scatter(48, y[s], color=plt.gca().lines[-1].get_color())\n",
    "  plt.title(fn)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9ARiWSS91kO"
   },
   "source": [
    "# 1. Unsupervised Learning Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning technique different from supervised ones from the fact that data are not labelled.\n",
    "\n",
    "We do not aim at fitting a mapping from $X$ to $y$, but to understand pattern in the data cloud $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afROKfT591kS"
   },
   "source": [
    "## 1. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the example that we will use to make the theory more concrete. We will take a dataset from from kaggle https://www.kaggle.com/datasets/miroslavsabo/young-people-survey?resource=download (already downloaded for you in the folder `data`)\n",
    "\n",
    "The datasets consists of the results of a survey about the music preferences of several students, arriving at the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T14:09:42.322377Z",
     "start_time": "2022-09-27T14:09:42.283713Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"data/responses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_columns=data.columns[:19]\n",
    "print(music_columns)\n",
    "music_data=data[music_columns].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers are of course correlated and we expect to have typical patterns recurring, that we define as people liking similar types of songs. \n",
    "\n",
    "The patterns may be also mixing, for e.g. a class of people may like classic `Pop` and `Reggae`, but not `Latino`. An other class may like `Latino` and `Reggae`, but not `Pop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA will help to find these typical patterns and their number in a data driven fashion. As we will see these patterns will naturally appear when trying to compress data in a lower dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXqNWQWT91kU"
   },
   "source": [
    "### Theory overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start looking at PCA from the point of view of `dimensionaliy reduction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVw0KP4_91kW"
   },
   "source": [
    "**Objective:** PCA is used for dimensionality reduction when we have a large number $D$ of features with non-trivial intercorrelation ( data redundancy ) and to isolate relevant features. The number of features $D$ defines the original dimension of the dataset. Each sample defines a vector of dimensionality $D$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    QUESTION: what are the starting vectors in our survey dataset? How many do we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T12:13:53.622053Z",
     "start_time": "2022-09-27T12:13:53.611428Z"
    }
   },
   "source": [
    "PCA provides a new set of $M$ uncorrelated features for every data point, with $M \\le D$. The new features are:\n",
    "\n",
    "- a linear combination of the original ones ; \n",
    "- uncorrelated between each other ; \n",
    "\n",
    "If $M \\ll D$ we get an effective dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T12:17:23.351734Z",
     "start_time": "2022-09-27T12:17:23.345972Z"
    }
   },
   "source": [
    "    QUESTION: Does the number of data points changes after applying PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnnFBIaf91kX"
   },
   "source": [
    "**Methods:** Each data point indexed by $p=1..N$ can be seen as an element $\\mathbf{x}_p \\in \\mathbf{R}^D$. Note that we can define an addition element-wise between the data-points (vector addition).\n",
    "\n",
    "The variance of the data-cloud measures the spread around its centroid:\n",
    "\n",
    "$$S^2=\\frac{1}{N}\\sum_{p=1}^{N}  | \\mathbf{x}_p - \\mathbf{\\overline{x}}|^2$$\n",
    "$$\\mathbf{\\overline{x}}=\\frac{1}{N}\\sum_{p=1}^{N} \\mathbf{x}_p$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T12:22:16.892832Z",
     "start_time": "2022-09-27T12:22:16.886499Z"
    }
   },
   "source": [
    "We fix a number $1\\le k \\le D$ and consider a subspace $V_k$ of dimension $k$. To visualize at first consider $D=3$ (our space), and $k=1,2$. $k=1$ takes into considerations lines, $k=2$ planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $V_k$ we can perform the following steps:\n",
    "\n",
    "1. Projekt each data point  $\\mathbf{x}_p$ onto $V_k$, leading to points $\\mathbf{x}^k_p$. Projecting a point $\\mathbf{x}_p$ is equivalent to getting the point $\\mathbf{x}^k_p$ belonging to $V_k$ and closest to $\\mathbf{x}_p$.\n",
    "\n",
    "2. Use the previous formula to evaluate $S^{2,k}$, the spread of the projected data cloud.\n",
    "\n",
    "This is exemplified below in the left figure.\n",
    "\n",
    "Now, depending on $V_k$ we get a difference variance. PCA chooses $V_k$ such that the variance $S^{2,k}$ is maximized, as shown in the picture on the right. The nice thing is that the subspace $V_k$ can be evaluated analytically, given any data cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwbKF5LZijSQ"
   },
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/pca-theory.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA describes subspaces (lines,planes, and their high dimensional generalizations) as \"generated\" by a sequence of vectors $v_1,...,v_k$. This is defined as the set of all the vectors:\n",
    "\n",
    "$$V_{\\{v_1,...,v_k\\}}=\\{ \\alpha_1 v_1+...+\\alpha_k v_k, \\alpha_1,..., \\alpha_k \\in \\mathbf{R} \\}$$\n",
    "\n",
    "PCA will therefore provide a sequence of vectors. Since many choices of $\\{v_1,...,v_k\\}$ could describe the same \"line\", \"plane\", etc. etc., it imposes an orthonormality condition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ySGaLWp91kZ"
   },
   "source": [
    "**Terminology and output of a PCA computation:** \n",
    "- `Principal components`: A sequence of orthonormal vectors $k_1,..,k_n$. We can interpret these vectors as the typical patterns found in the data, from increasing to decreasing probability of appearance. Any vector can be constructed as a linear combination of such patterns. Using only the first $m$ vectors we get a \"best approximation\".\n",
    "\n",
    "    \n",
    "- `Scores`: For every sample-point $p$, the new features are called scores are given by the component of $p$ along the $k$ vectors;  \n",
    "\n",
    "\n",
    "- `Reconstructed vector`: For every $k$, the projection of $V$ on $V_k$. This is the best approximation that we can get of the vector using only $k$ vectors (principal components).\n",
    "\n",
    "\n",
    "- `Explained variance`: For every k, the ratio between the variance of the reconstructed vectors and total variance. The number of components is chosen selecting an optimal k. The plot of the explained variance as a function of k is called a *scree plot*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    QUESTION: \n",
    "    What are the main differences between a vector $x_p$ and a principal component $k_n$? In particular:\n",
    "    1) how many do we have ?\n",
    "    2) what are their dimensions ?\n",
    "    \n",
    "    QUESTION: \n",
    "    What are the differences between $x_p$ and a score vector $s_p$? In particular:\n",
    "    1) how many do we have ?\n",
    "    2) what are their dimensions ?\n",
    "    \n",
    "    QUESTION: \n",
    "    1) How many reconstructed vectors do we have? What is their dimension ? Can we have a reconstructed vector with zero error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hF-FTX_P91ka"
   },
   "source": [
    "### Sklearn: implementation and usage of PCA.\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGwCp73k91kb"
   },
   "source": [
    "We start showing a two-dimensional example that can be easy visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoEWiYnL91ko"
   },
   "source": [
    "We load the datasets that we are going to use for the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_sample_data_pca()\n",
    "\n",
    "n_samples,n_dim=data.shape\n",
    "\n",
    "print('We have ',n_samples, 'samples of dimension ', n_dim)\n",
    "\n",
    "plt.figure(figsize=((5,5)))\n",
    "plt.grid()\n",
    "plt.plot(data[:,0],data[:,1],'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzvgzzfA91k8"
   },
   "source": [
    "The data set is almost one dimensional. PCA will confirm this result.\n",
    "\n",
    "As with most of sklearn functionalities, we need first to create a PCA object. We will use the object methods to perform PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T19:12:10.956955Z",
     "start_time": "2022-09-26T19:12:10.953963Z"
    },
    "id": "l6poli6o91k_"
   },
   "outputs": [],
   "source": [
    "pca=PCA(n_components=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju0UCpkP91lK"
   },
   "source": [
    "A call to the pca.fit method computes the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T19:12:12.057604Z",
     "start_time": "2022-09-26T19:12:12.034521Z"
    },
    "id": "ifWK32ME91lM"
   },
   "outputs": [],
   "source": [
    "pca.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZdfZbLv91lW"
   },
   "source": [
    "Now the pca.components_ attribute contains the principal components. We can print them alongside with the data and check that they constitute an orthonormal basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=((5,5)))\n",
    "plt.grid()\n",
    "plt.plot(data[:,0],data[:,1],'o')\n",
    "\n",
    "circle=plt.Circle((0, 0), 1.0, linestyle='--', color='red',fill=False)\n",
    "ax=plt.gca()\n",
    "ax.add_artist(circle)\n",
    "\n",
    "for vec in pca.components_:\n",
    "    plt.quiver([0], [0], [vec[0]], [vec[1]], angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-2,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q48o3r3t91li"
   },
   "source": [
    "The pca.explained_variance_ratio_ attribute contains the explained variance. In this case we see that already the first reconstructed vector explains 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T19:12:22.269942Z",
     "start_time": "2022-09-26T19:12:22.263717Z"
    },
    "id": "TwEopdTN91lk"
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfaA1wo391ls"
   },
   "source": [
    "To compute the reconstructed vectors for k=1 we first need to compute the scores and then multiply by the basis vectors:\n",
    "\n",
    "$\\mathbf x_{rec}=\\sum_i (\\mathbf x \\cdot \\mathbf v^{pr}_i) \\mathbf v^{pr}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T19:12:24.347892Z",
     "start_time": "2022-09-26T19:12:24.343929Z"
    },
    "id": "enRkHTyO91lu"
   },
   "outputs": [],
   "source": [
    "k=1\n",
    "scores=pca.transform(data)\n",
    "res=np.dot(scores[:,:k], pca.components_[:k,:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=((5,5)))\n",
    "plt.plot(res[:,0],res[:,1],'o')\n",
    "plt.plot(data[:,0],data[:,1],'o')\n",
    "\n",
    "for a,b,c,d in zip(data[:,0],data[:,1],res[:,0],res[:,1]) :\n",
    "    plt.plot([a,c],[b,d],'-', linestyle = '--', color='red')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.xlim(-1.0,1.0)\n",
    "plt.ylim(-1.0,1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZlmDioD91mD"
   },
   "source": [
    "The same procedure is followed for high dimensional datasets. Here we generate random data which lies almost on a 6-dimensional subspace. The resulting scree plot can be used to find this result in a semi-automatic fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redo the same now with our survey dataset, to review the concepts again and think \"high\"-dimensionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T14:44:10.677074Z",
     "start_time": "2022-09-27T14:44:10.652371Z"
    }
   },
   "outputs": [],
   "source": [
    "pca=PCA() \n",
    "pca.fit(music_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_,'-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pca.components_.transpose(), \n",
    "                  columns = [f'V_{i+1}' for i in range(len(music_columns))], \n",
    "                  index=music_columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vector in ['V_1','V_2','V_3','V_4']:\n",
    "    plt.figure()\n",
    "    plt.title(vector)\n",
    "    plt.plot(np.arange(len(music_columns)),list(df[vector]),'-o')\n",
    "    _=plt.xticks(np.arange(len(music_columns)),music_columns, rotation=90)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T12:40:33.906396Z",
     "start_time": "2022-09-27T12:40:33.902364Z"
    }
   },
   "source": [
    "### EXERCISE 1 : Find the dimensionality of the hidden dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise you will take a custom high dimensional dataset and try to find its dimensionality \n",
    "\n",
    "# 1. Load the data using the function data=load_multidimensional_data_pca() , check the dimensionality of the data and plot them.\n",
    "data=load_multidimensional_data_pca(n_data=40, n_vec=6, dim=20, eps=0.5)\n",
    "data.shape\n",
    "n_samples,n_dim=data.shape\n",
    "print('We have ',n_samples, 'samples of dimension ', n_dim)\n",
    "\n",
    "# 2. Define a PCA object and perform the PCA fitting.\n",
    "pca=PCA()\n",
    "pca.fit(data)\n",
    "\n",
    "# 3. Check the explained variance ratio and select best number of components.\n",
    "#print(pca...)\n",
    "#plt.plot(...)\n",
    "data_transformed=pca.transform(data)\n",
    "data_transformed\n",
    "#plt.plot(pca.explained_variance_ratio_,'-o')\n",
    "#plt.title('Scree plot')\n",
    "#plt.ylabel('Percentage of explained variance')\n",
    "#plt.grid()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nJsEk2R91nM"
   },
   "source": [
    "### EXERCISE 2 : Find the hidden drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this exercise you will take a high dimensional dataset, find the optimal number of principal components \n",
    "# and see what plot do we get if we visualize in a 2D plot the first two scores.\n",
    "\n",
    "# 1. Load the data using the function data=load_ex2_data_pca(seed=1235) , check the dimensionality of the data and plot them.\n",
    "data=load_ex2_data_pca(seed=1235)\n",
    "n_samples,n_dim=data.shape\n",
    "print('We have ',n_samples, 'samples of dimension ', n_dim)\n",
    "\n",
    "# 2. Define a PCA object and perform the PCA fitting.\n",
    "pca=PCA()\n",
    "pca.fit(data)\n",
    "\n",
    "# 3. Check the explained variance ratio and select best number of components.\n",
    "plt.plot(pca.explained_variance_ratio_,'-o')\n",
    "plt.title('Scree plot')\n",
    "plt.ylabel('Percentage of explained variance')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 4. Plot the reconstructed vectors for the best value of data=...\n",
    "k=2\n",
    "data_transformed=pca.transform(data)\n",
    "data_transformed\n",
    "plt.plot(data_transformed[:,0],data_transformed[:,2],'o')\n",
    "plt.xlabel('First component')\n",
    "plt.ylabel('Second component')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3 (bonus) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T15:16:35.115619Z",
     "start_time": "2022-09-27T15:16:35.112616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Explore more the survey dataset. Suppose you have studied the first vectors and found \n",
    "# some 4 types of music advertisment, targetted to the 4 classes of clients.\n",
    "\n",
    "# Given a customer, how would you choose which advertisement to send him?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3AtpLmQef_R"
   },
   "source": [
    "### Final comments:\n",
    "\n",
    "PCA is able therefore to make this mapping:\n",
    "\n",
    "$(x_1,...,x_D) \\rightarrow (y_1,..,y_M)$\n",
    "\n",
    "Here we focused on data compression, but it is also very important that $y_1,...,y_M$ are uncorrolated for interpratibility purposes. Being uncorrelated means (roughly) that in our dataset we can change one variable without affecting the others. The dimensions 1,...,M are often therefore more interpretable and providing more information.\n",
    "\n",
    "See e.g. a similar application here:\n",
    "\n",
    "\"Principal component analysis of dietary and lifestyle patterns in relation to risk of subtypes of esophageal and gastric cancer\", https://pubmed.ncbi.nlm.nih.gov/21435900/\n",
    "\n",
    ", where each data point $x$ is an answer from a questionnaire of food. The principal components are than typical \"patterns\" of answers that are uncorrlated, have a look at table 2, and if you want read the whole data story :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEK5-Ksg91nw"
   },
   "source": [
    "## 2. Data visualization and embedding in low dimensions ( t-SNE / UMAP )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rP48tTL91nz"
   },
   "source": [
    "### Theory overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_Jer5L-91n0"
   },
   "source": [
    "PCA is a linear embedding technique where the scores are a linear function of the original variables. This forces the number of principal components to be used to be high, if the manifold is highly non-linear. Curved manifolds need to be embedded in higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX4zvhaN91n2"
   },
   "source": [
    "Other `non-linear` embedding techniques start from a local description of the environment of each sample point in the original space:\n",
    "\n",
    "- `t-Sne` uses a `statistical description` of the environment of a sample point ;\n",
    "- `UMAP` describes the `topology` of the environment through a generalized \"triangulation\" (simplex decomposition) ;\n",
    "\n",
    "The projection on the low-dimensional space is optimized in order to match as much as possible the description of the local environment. \n",
    "\n",
    "It is not the goal of this introduction to discuss the derivation of such approaches, which can be found in the references:\n",
    "\n",
    "https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1802.03426.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhP5EIc791n7"
   },
   "source": [
    "Instead, the following, we will show how to apply practically these dimensionality reductions techniques. Keep in mind that the embedding is given by an iterative solution of a minimization problem and therefore the results may depend on the value of the random seed, especially for t-SNE visualizazions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwIggNFN91n8"
   },
   "source": [
    "### Utilization in Python and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP5RbzRa91n9"
   },
   "source": [
    "To begin with, we create a t-SNE object that we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T19:17:17.498928Z",
     "start_time": "2022-09-26T19:17:17.495285Z"
    },
    "id": "BMZtUAyR91n9"
   },
   "outputs": [],
   "source": [
    "tsne_model = TSNE(perplexity=30, n_components=2, learning_rate=200, early_exaggeration=4.0,init='pca', \n",
    "                      n_iter=2000, random_state=2233212, metric='euclidean', verbose=100 )\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=30, n_components=2, random_state=1711)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-iq56Yn91oD"
   },
   "source": [
    "### Example 1: Exercise 3 Cont'd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFZ_RAwj91oE"
   },
   "source": [
    "We will first visualize our multi-dimensional heart using t-SNE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T19:26:26.119196Z",
     "start_time": "2022-09-26T19:26:25.619284Z"
    },
    "id": "8TFExSbC91oF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data= load_ex2_data_pca(seed=1235, n_add=20)\n",
    "\n",
    "tsne_model = TSNE(perplexity=30, n_components=2, learning_rate=200, early_exaggeration=4.0,init='pca', \n",
    "                      n_iter=300, random_state=2233212, metric='euclidean', verbose=1 )\n",
    "\n",
    "tsne_heart = tsne_model.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T19:25:22.637451Z",
     "start_time": "2022-09-26T19:25:22.473134Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from utils.routines import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                   init='random', perplexity=3, verbose=1).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T19:25:31.237944Z",
     "start_time": "2022-09-26T19:25:30.300387Z"
    }
   },
   "outputs": [],
   "source": [
    "data= load_ex2_data_pca(seed=1235, n_add=20)\n",
    "\n",
    "tsne_model = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3, verbose=1)\n",
    "\n",
    "tsne_heart = tsne_model.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_heart[:,0],tsne_heart[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0R8p_0E91oR"
   },
   "source": [
    "And using UMAP :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUU2YauU91oT"
   },
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_neighbors=30, n_components=2, random_state=1711)\n",
    "\n",
    "umap_hart = umap_model.fit_transform(data)\n",
    "plt.scatter(umap_hart[:, 0], umap_hart[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kot3Prcv91oi"
   },
   "source": [
    "### Example 2: Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "n_examples = 5000\n",
    "data=train_images[:n_examples,:].reshape(n_examples,-1)\n",
    "data=data/255\n",
    "\n",
    "labels=train_labels[:n_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDeL_V2a91op",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# not to run on COLAB\n",
    "\n",
    "# tsne_model = TSNE(perplexity=10, n_components=2, learning_rate=200,\n",
    "#                   early_exaggeration=4.0,init='pca', \n",
    "#                   n_iter=2000, random_state=2233212, \n",
    "#                   metric='euclidean', verbose=100, n_jobs=1)\n",
    "\n",
    "# tsne_mnist = tsne_model.fit_transform(data)\n",
    "\n",
    "# plt.scatter(tsne_mnist[:,0],tsne_mnist[:,1],c=labels,s=10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRwwHbqpt9n0"
   },
   "source": [
    " | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/t_sne_mnist.png\" width=\"100%\"/> | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/mnist.png\" width=\"100%\"/> |\n",
    " |  -----:| -----:|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_neighbors=10, n_components=2, random_state=1711)\n",
    "umap_mnist = umap_model.fit_transform(data)\n",
    "plt.scatter(umap_mnist[:, 0], umap_mnist[:, 1], c=labels, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPOM-hCV91pB"
   },
   "source": [
    "### Example 3: Fashion_Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uq3DPRiV91pF"
   },
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "n_examples = 5000\n",
    "data=train_images[:n_examples,:].reshape(n_examples,-1)\n",
    "data=data/255\n",
    "\n",
    "labels=train_labels[:n_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ytrl7jyC91pR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# not to run on COLAB\n",
    "\n",
    "# tsne_model = TSNE(perplexity=50, n_components=2, learning_rate=200, early_exaggeration=4.0,init='pca', \n",
    "#                      n_iter=1000, random_state=2233212, metric='euclidean', verbose=100 )\n",
    "\n",
    "# tsne_fmnist = tsne_model.fit_transform(data)\n",
    "\n",
    "# plt.scatter(tsne_fmnist[:,0],tsne_fmnist[:,1],c=labels,s=10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T15:07:26.743568Z",
     "start_time": "2022-09-27T15:07:26.738108Z"
    },
    "id": "UUnsyxnet9n9"
   },
   "source": [
    " | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/t_sne_fmnist.png\" width=\"100%\"/> | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/fmnist.png\" width=\"100%\"/> \n",
    " |  -----:| -----:|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xq3vZFJy91p8"
   },
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_neighbors=50, n_components=2, random_state=1711)\n",
    "umap_fmnist = umap_model.fit_transform(data)\n",
    "plt.scatter(umap_fmnist[:, 0], umap_fmnist[:, 1], c=labels, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz2LEGDP91qM"
   },
   "source": [
    "### Example 4: House prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eycvatmq91qO"
   },
   "outputs": [],
   "source": [
    "data=house_prices_dataset_normed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82secxju91qZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# not to run on COLAB\n",
    "\n",
    "#tsne_model = TSNE(perplexity=30, n_components=2, learning_rate=200,\n",
    "#                  early_exaggeration=4.0,init='pca', n_iter=1000,\n",
    "#                  random_state=2233212, metric='euclidean', verbose=100)\n",
    "\n",
    "#tsne_houses = tsne_model.fit_transform(data)\n",
    "\n",
    "#plt.scatter(tsne_houses[:,0],tsne_houses[:,1],s=20)\n",
    "#plt.savefig('t_sne_houses.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESjUplVMt9oF"
   },
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/t_sne_houses.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWeCP6pN91qr"
   },
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_neighbors=30, n_components=2, random_state=1711)\n",
    "umap_houses = umap_model.fit_transform(data)\n",
    "plt.scatter(umap_houses[:, 0], umap_houses[:, 1], s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B3wGs2Q91rE"
   },
   "source": [
    "**Message:** Visualization techniques are useful for having an initial grasp of multi-dimensional datasets and guide further analysis and the choice of the modelling data strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78xdYF9-B-jA"
   },
   "source": [
    "**Caveats:** \n",
    "- available t-SNE implementations may vary a lot in terms of performance. Computational time can be reduced performing PCA before a t-SNE projection\n",
    "\n",
    "- UMAP, thanks to the algorithm being amanable to clever initializations and optimization schemes, offers great stability and scaling properties\n",
    "\n",
    "- UMAP, even if starting from a local picture, is generally more able to spread apart different clusters\n",
    "\n",
    "- The result of an embedding may depend on the values of the metaparameters. One should try to see how the final embedding changes in order to get to a complete picture"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ITfbaOgfYNsq",
    "7-CGSS2OZKHD",
    "Bxtv48o-F1Ku",
    "l582Sr0_WGXj",
    "EHZ-hHGuY5aG",
    "puQNgKN0wS7H",
    "vhhycm2S6wbz",
    "afROKfT591kS",
    "1B7g_pGO91md"
   ],
   "name": "Course_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "a421f863b9c1462b4a0034deb3cdef3c95c871fa51f74595ff57f35c4e240ca6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
